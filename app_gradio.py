import asyncio
import httpx
from concurrent.futures import ThreadPoolExecutor

import re
import gradio as gr
import requests
import json
from typing import Generator
import threading
from queue import Queue
import time
import os
import glob
from pathlib import Path
from pprint import pformat

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
from data_processing import load_dataset, load_corpus_text, load_start_nodes, convert_paths
from io_utils import read_json

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

http_client = httpx.AsyncClient(timeout=300.0)

def process_text_stream(text: str, max_iterations: int, temperature: float, top_p: float, 
                       reranking: bool, interpreting: bool, functions: list, output_file: str = None, start_node_id: str = None):
    """Process text using the streaming API endpoint with optional start node"""
    
    # Validate input
    if '<predict_kb>' not in text or '</predict_kb>' not in text:
        yield "‚ùå –û—à–∏–±–∫–∞: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–≥–∏ <predict_kb>...</predict_kb>", ""
        return
    
    valid_functions = ["get_hyponyms", "get_hypernyms"]
    functions = [f for f in functions if f in valid_functions]
    
    if not functions:
        functions = ["get_hyponyms"]
    
    api_url = "http://localhost:8500"
    
    # Prepare request with optional start node
    endpoint = f"{api_url.rstrip('/')}/predict/stream"
    payload = {
        "text": text,
        "max_iterations": max_iterations,
        "temperature": temperature,
        "top_p": top_p,
        "reranking": reranking,
        "interpreting": interpreting,
        "functions": functions,
        "output_file": output_file
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if start_node_id:
        payload["start_node_id"] = start_node_id
    
    # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–æ–π –∂–µ...
    
    process_log = ""
    final_result = ""

    logger.info(f'Sending request with payload {pformat(payload)}')
    
    try:
        # Make streaming request
        with requests.post(endpoint, json=payload, stream=True, timeout=300) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    
                    # Parse SSE format
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # Remove 'data: ' prefix
                        
                        try:
                            data = json.loads(data_str)
                            event_type = data.get('type')
                            
                            if event_type == 'iteration':
                                iteration_info = f"\n{'='*54}\nüîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {data['iteration']}\n{'='*54}\n\n"
                                process_log += iteration_info
                                yield process_log, final_result
                            
                            elif event_type == 'thought':
                                thought = f"üí≠ –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:\n{data['content']}\n\n"
                                process_log += thought
                                yield process_log, final_result
                            
                            elif event_type == 'tool_call':
                                tool_info = f"üîß –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: {data['function']}\n"
                                tool_info += f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã: {json.dumps(data['args'], ensure_ascii=False)}\n"
                                tool_info += f"–£–∑–µ–ª: {data['node_name']}\n\n"
                                process_log += tool_info
                                yield process_log, final_result
                            
                            elif event_type == 'tool_response':
                                # Display the markdown formatted function response
                                response_info = f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏:\n{data['content']}\n"
                                process_log += response_info
                                yield process_log, final_result
                            
                            elif event_type == 'tracking_saved':
                                tracking_info = f"üíæ –î–∞–Ω–Ω—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data['file']} (–≤—ã–±—Ä–∞–Ω–æ —É–∑–ª–æ–≤: {data['selections_count']})\n"
                                process_log += tracking_info
                                yield process_log, final_result
                            
                            elif event_type == 'final':
                                final_result = f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n\n{data['result']}"
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                                if 'tracking_data' in data:
                                    tracking_data = data['tracking_data']
                                    tracking_info = f"\n\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:\n"
                                    tracking_info += f"‚Ä¢ –¶–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ: {tracking_data.get('target_word', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
                                    tracking_info += f"‚Ä¢ –í—ã–±—Ä–∞–Ω–æ —É–∑–ª–æ–≤: {tracking_data.get('total_selections', 0)}\n"
                                    
                                    if tracking_data.get('selected_synsets'):
                                        tracking_info += f"‚Ä¢ –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Å–∏–Ω—Å–µ—Ç—ã:\n"
                                        for synset in tracking_data['selected_synsets']:
                                            tracking_info += f"  - {synset['synset_id']} ({synset['function']})\n"
                                    
                                    final_result += tracking_info
                                
                                process_log += f"\n{final_result}\n"
                                process_log += f"\n{'='*54}\n‚úîÔ∏è –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω\n{'='*54}\n"
                                yield process_log, final_result
                                return
                            
                            elif event_type == 'error':
                                error_msg = f"‚ùå –û—à–∏–±–∫–∞: {data['message']}\n"
                                process_log += error_msg
                                yield process_log, final_result
                                return
                        
                        except json.JSONDecodeError:
                            continue
        
    except requests.exceptions.Timeout:
        yield "‚ùå –û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞", ""
    except requests.exceptions.ConnectionError:
        yield f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ API –∑–∞–ø—É—â–µ–Ω –Ω–∞ {api_url}", ""
    except requests.exceptions.HTTPError as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ HTTP: {e.response.status_code} - {e.response.text}", ""
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞: {str(e)}", ""


def safe_file_path(file_obj):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
    if file_obj is None:
        return None
    if hasattr(file_obj, 'name'):
        return file_obj.name
    return str(file_obj)


def process_dataset_item(
        dataset_path: str, corpus_folder: str, word: str, 
        max_iterations: int, temperature: float, top_p: float, 
        reranking: bool, interpreting: bool, functions: list, 
        num_processes: int, start_nodes_folder: str, max_n_starting_nodes: int,
        parallel_mode: str, output_file: str = None
    ):
    """Process a specific word from dataset using corpus text"""
    try:
        # Load corpus text for the word
        dataset = convert_paths(load_dataset(dataset_path), 0)
        texts = [load_corpus_text(corpus_folder, item_path) for item_path in dataset[word]]
        if not texts:
            yield f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–≤–∞: {word}", ""
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if not output_file:
            timestamp = int(time.time())
            output_file = f"tracking_results/single_word_{word}_{timestamp}.json"
        
        # Process using the streaming function
        for results in run_parallel_analysis(
            word, texts, max_iterations, temperature, top_p, reranking, interpreting, functions, output_file,
            num_processes, start_nodes_folder, max_n_starting_nodes, parallel_mode
        ):
            yield results
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–≤–∞ '{word}': {str(e)}", ""


def get_dataset_info(dataset_path: str):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    if not dataset_path or not os.path.exists(dataset_path):
        return "–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –≤—ã–±—Ä–∞–Ω –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω", gr.update(choices=[], interactive=False), gr.update(maximum=1, value=1)
    
    try:
        dataset = load_dataset(dataset_path)
        words = list(dataset.keys())
        max_samples = len(words)
        info = f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(words)} —Å–ª–æ–≤"
        
        return (
            info, 
            gr.update(choices=[], interactive=True, allow_custom_value=True),
            gr.update(maximum=max_samples, value=1, interactive=True),
            gr.update(maximum=max_samples, value=max_samples, interactive=True),
            max_samples
        )
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}", gr.update(choices=[], interactive=False), gr.update(maximum=1, value=1), 1, 1

def search_words_in_dataset(dataset_path: str, search_query: str):
    """Search for words in dataset that match the query"""
    if not dataset_path or not search_query:
        return gr.update(value=search_query.upper(), choices=[])
    
    try:
        dataset = load_dataset(dataset_path)
        words = list(dataset.keys())
        
        # –ü–æ–∏—Å–∫ —Å–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∑–∞–ø—Ä–æ—Å (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
        search_query = search_query.upper().strip()
        matching_words = [word for word in words if search_query in word.upper()]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ 50 —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        matching_words = matching_words[:50]
        
        return gr.update(value=search_query.upper(), choices=matching_words)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {e}")
        return gr.update(value=search_query.upper(), choices=[])

def validate_word_in_dataset(dataset_path: str, word: str):
    """Validate that the word exists in dataset"""
    if not dataset_path or not word:
        return "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ª–æ–≤–æ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"
    
    try:
        dataset = load_dataset(dataset_path)
        if word in dataset:
            return f"‚úÖ –°–ª–æ–≤–æ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
        else:
            # –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
            words = list(dataset.keys())
            similar = [w for w in words if word.upper() in w.upper() or w.upper() in word.upper()][:5]
            if similar:
                return f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: {', '.join(similar)}"
            else:
                return f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}"

def load_word_text_from_corpus(dataset_path, corpus_folder: str, word: str, index: int = 0):
    """Load text for specific word from corpus"""
    if not dataset_path or not corpus_folder or not word:
        return "", gr.update(), 0, gr.update()
    
    dataset = load_dataset(dataset_path)
    paths = convert_paths({word: dataset[word]})[word]
    n_texts = len(paths)
    
    if paths:
        if index >= n_texts:
            index = 0
        elif index < 0:
            index = n_texts - 1
        try:
            text = load_corpus_text(corpus_folder, paths[index])
            logger.debug(f'–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç {text}')
            interactive = len(paths) > 1
            return text, gr.update(interactive=interactive), str(index + 1), gr.update(interactive=interactive)
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}", gr.update(), '0', gr.update()
    
    return f"‚ùå –î–ª—è —Å–ª–æ–≤–∞ {word} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ", gr.update(), '0', gr.update()

async def process_dataset_batch_async(dataset_file, corpus_folder, sample_start, max_samples, num_processes, batch_size,
                                     max_iterations, temperature, top_p, reranking, interpreting, functions, 
                                     start_nodes_path, max_n_starting_nodes, parallel_mode, progress=None):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"""
    start_time = int(time.time())

    if not dataset_file or not corpus_folder:
        return "‚ùå –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø–∞–ø–∫—É –∫–æ—Ä–ø—É—Å–∞"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    file_path = safe_file_path(dataset_file)
    dataset = convert_paths(load_dataset(file_path), 0)
    start_nodes_dict = load_start_nodes(start_nodes_path) if start_nodes_path and max_n_starting_nodes > 0 else {}
    
    words_to_process = list(dataset.keys())[sample_start-1:sample_start+max_samples-1]
    total_tasks = 0
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á
    for word in words_to_process:
        if word in start_nodes_dict and max_n_starting_nodes > 0 and parallel_mode == "–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º":
            word_nodes = start_nodes_dict[word][:max_n_starting_nodes]
            total_tasks += len(word_nodes)
        elif max_n_starting_nodes > 0 and parallel_mode == "–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º":
            logger.warning(f'Word {word} not found in start nodes list ({list(start_nodes_dict.keys())[0]},...)')
        elif parallel_mode == "–ø–æ —Ç–µ–∫—Å—Ç–∞–º" and dataset[word]:
            total_tasks += len(dataset[word])
        elif parallel_mode == "–ø–æ —Ç–µ–∫—Å—Ç–∞–º":
            logger.warning(f'No texts for {word} found in the dataset')
        else:
            total_tasks += num_processes
    
    logger.info(f"–ó–∞–ø—É—Å–∫ {total_tasks} –∑–∞–¥–∞—á –¥–ª—è {len(words_to_process)} —Å–ª–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    semaphore = asyncio.Semaphore(min(total_tasks, 10))  # –ú–∞–∫—Å–∏–º—É–º 10 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    async def process_single_task(word, text, start_node_id, task_id):
        async with semaphore:
            try:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
                payload = {
                    "text": text,
                    "max_iterations": max_iterations,
                    "temperature": temperature,
                    "top_p": top_p,
                    "reranking": reranking,
                    "interpreting": interpreting,
                    "functions": functions,
                    "output_file": f"tracking_results/batch_{start_time}/async_word_{word}_[{task_id}].json"
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª –µ—Å–ª–∏ –µ—Å—Ç—å
                if start_node_id:
                    payload["start_node_id"] = start_node_id
                
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ API
                response = await http_client.post(
                    "http://localhost:8500/predict",
                    json=payload
                )
                response.raise_for_status()
                
                result_data = response.json()
                return {
                    "word": word,
                    "result": result_data.get("result"),
                    "iterations": result_data.get("iterations"),
                    "task_id": task_id,
                    "start_node_id": start_node_id
                }
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ {task_id} –¥–ª—è —Å–ª–æ–≤–∞ {word}: {e}")
                return {
                    "word": word,
                    "error": str(e),
                    "task_id": task_id,
                    "start_node_id": start_node_id
                }
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
    if progress:
        progress(0.1, f"–°–æ–∑–¥–∞–Ω–æ {total_tasks} –∑–∞–¥–∞—á –¥–ª—è {len(words_to_process)} —Å–ª–æ–≤")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    completed_tasks = 0
    results = []

    tasks = []
    
    for w, word in enumerate(words_to_process):
        if word in start_nodes_dict and max_n_starting_nodes > 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∑–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª—ã
            word_nodes = start_nodes_dict[word][:max_n_starting_nodes]
        else:
            word_nodes = []

        if parallel_mode == '–ø–æ —Ç–µ–∫—Å—Ç–∞–º':
            for t, path in enumerate(dataset[word]):
                text = load_corpus_text(corpus_folder, path)
                if not text or "‚ùå" in text:
                    continue
                if word_nodes:
                    for n, node_id in enumerate(word_nodes):
                        task_id = f'{w}_{t}_{n}_0'
                        tasks.append(process_single_task(word, text, node_id, task_id))
                else:
                    task_id = f'{w}_{t}__0'
                    tasks.append(process_single_task(word, text, None, task_id))
        else:
            text = load_corpus_text(corpus_folder, dataset[word][0])
            if not text or "‚ùå" in text:
                continue
            if parallel_mode == '–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º':
                for n, node_id in enumerate(word_nodes):
                    task_id = f'{w}_0_{n}_0'
                    tasks.append(process_single_task(word, text, node_id, task_id))
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
                for i in range(num_processes):
                    task_id = f'{w}_0__{i}'
                    tasks.append(process_single_task(word, text, None, task_id))

    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    for i in range(0, len(tasks), batch_size):
        batch_tasks = tasks[i:i+batch_size]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        results.extend(batch_results)
        
        completed_tasks += len(batch_tasks)
        if progress:
            progress_value = 0.1 + (completed_tasks / total_tasks) * 0.8
            progress(progress_value, f"–í—ã–ø–æ–ª–Ω–µ–Ω–æ {completed_tasks}/{total_tasks} –∑–∞–¥–∞—á")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    timestamp = int(time.time())
    output_file = f"test_results/async_batch_{len(words_to_process)}words_{timestamp}.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–ª–æ–≤–∞–º
    results_by_word = {}
    for result in results:
        if isinstance(result, dict) and 'word' in result:
            word = result['word']
            if word not in results_by_word:
                results_by_word[word] = []
            results_by_word[word].append(result)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_by_word, f, ensure_ascii=False, indent=2)
    
    if progress:
        progress(1.0, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    return f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\nüìä –í—ã–ø–æ–ª–Ω–µ–Ω–æ {len(results)} –∑–∞–¥–∞—á –¥–ª—è {len(words_to_process)} —Å–ª–æ–≤\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_file}"

def process_dataset_batch(dataset_file, corpus_folder, sample_start, max_samples, num_processes, batch_size,
                         max_iterations, temperature, top_p, reranking, interpreting, functions,
                         start_nodes_path, max_n_starting_nodes, parallel_mode, progress=gr.Progress()):
    """Wrapper –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤ –Ω–æ–≤–æ–º event loop
    def run_async():
        try:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            return loop.run_until_complete(
                process_dataset_batch_async(
                    dataset_file, corpus_folder, sample_start, max_samples, num_processes, batch_size,
                    max_iterations, temperature, top_p, reranking, interpreting, functions,
                    start_nodes_path, max_n_starting_nodes, parallel_mode, progress
                )
            )
        finally:
            loop.close()
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å Gradio
    with ThreadPoolExecutor() as executor:
        future = executor.submit(run_async)
        return future.result() 
    
def get_start_nodes_info(start_nodes_path: str):
    """Get information about start nodes file"""
    if not start_nodes_path or not os.path.exists(start_nodes_path):
        return "–§–∞–π–ª —Å–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º–∏ —É–∑–ª–∞–º–∏ –Ω–µ –≤—ã–±—Ä–∞–Ω –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω", gr.update(), gr.update()
    
    try:
        content = read_json(start_nodes_path)
        total_words = len(content)
        total_nodes = sum(len(nodes) for nodes in content.values())
        return f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {total_words} —Å–ª–æ–≤, {total_nodes} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤", gr.update(interactive=True, maximum=3), gr.update(choices=['–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º']+parallel_mode.choices, value='–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º')
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}", gr.update(), gr.update()


# Example text
example_text = '''–ö–∞–∂–¥–æ–µ –ª–µ—Ç–æ –≥—Ä—É–ø–ø—ã —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Å–µ–±—è –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ –ø–æ–∏—Å–∫–∏ —Å–Ω–µ–≥–∞ –∏ –ª—å–¥–∞. –ß–∞—â–µ –≤—Å–µ–≥–æ –∏—Ö –Ω–∞–∑—ã–≤–∞—é—Ç –∞–ª—å–ø–∏–Ω–∏—Å—Ç—ã, –∏ –æ–Ω–∏ –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è –≥–æ–¥–∞ –Ω–µ –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ—Å–µ—á—å –ª–µ–¥–Ω–∏–∫ –∏–ª–∏ —Ç—Ä–æ–ø–∏—Ç—å –ø–æ —Å–Ω–µ–≥—É –¥–æ –≤–µ—Ä—à–∏–Ω—ã. –•—Ä–∞–±—Ä—ã–µ –ø—Ä–æ—Ñ–∏ –¥–∞–∂–µ –≥–æ—Ç–æ–≤—ã –ª–µ–∑—Ç—å –ø–æ —Å–∫–∞–ª–∞–º —Å–æ –ª—å–¥–æ–º, –≤—ã–±–∏—Ä–∞—è –∑–∞–ø—Ä–µ–¥–µ–ª—å–Ω–æ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã. –ì–æ—Ä–Ω—ã–µ —Ç—É—Ä–∏—Å—Ç—ã —Ç–æ–∂–µ —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –≥—É–ª—è—é—Ç —Å—Ä–µ–¥–∏ –≤–µ—á–Ω–æ–π –º–µ—Ä–∑–ª–æ—Ç—ã –Ω–∞ –≤—ã—Å–æ—Ç–∞—Ö –±–æ–ª–µ–µ 4000 –º–µ—Ç—Ä–æ–≤ –Ω–∞–¥ —É—Ä–æ–≤–Ω–µ–º –º–æ—Ä—è. –ò –≤—Å–µ–º –∏–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ–µ —Å—Ü–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ —Å–∫–æ–ª—å–∑–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª—å–¥–∞.

–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π –∫—É–∑–Ω–µ—Ü –∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–æ–π –∞–ª—å–ø–∏–Ω–∏—Å—Ç—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ì–µ–Ω—Ä–∏ –ì—Ä–∏–≤–µ–ª—å –±–æ–ª–µ–µ 100 –ª–µ—Ç –Ω–∞–∑–∞–¥ —Å–Ω–∞–±–¥–∏–ª –æ–¥–Ω–∏—Ö –∏–∑ –ø–µ—Ä–≤—ã—Ö –≤–æ—Å—Ö–æ–¥–∏—Ç–µ–ª–µ–π –ø—Ä–æ–æ–±—Ä–∞–∑–æ–º —Ç–æ–≥–æ, —á—Ç–æ —Å–µ–π—á–∞—Å –Ω–∞–∑—ã–≤–∞—é—Ç <predict_kb>–∫–æ—à–∫–∞–º–∏</predict_kb>. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –±—ã–ª–∏ –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ä—è–¥ —Å–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Å–∫–æ–± —Å –∑–∞–æ—Å—Ç—Ä—ë–Ω–Ω—ã–º–∏ —à–∏–ø–∞–º–∏ –∏ —Ä–µ–º–Ω—è–º–∏ –¥–ª—è –∫—Ä–µ–ø–ª–µ–Ω–∏—è. –û–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ —Ç–∞–∫—Ç–∏–∫—É –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Å–Ω–µ–∂–Ω–æ-–ª–µ–¥–æ–≤–æ–º—É —Å–∫–ª–æ–Ω—É –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–æ–≤.

–° —Ç–µ—Ö –≤—Ä–µ–º—ë–Ω –º–æ–¥–µ–ª–∏ –∑–∞–º–µ—Ç–Ω–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–ª–∏, –Ω–æ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —ç—Ç–æ –∏–∑–¥–µ–ª–∏—è –∏–∑ –º–µ—Ç–∞–ª–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–ø—è—Ç—Å—è –∫ –±–æ—Ç–∏–Ω–∫–∞–º, –≤–≥—Ä—ã–∑–∞—é—Ç—Å—è –≤ –ª—ë–¥ –∏ –¥–µ—Ä–∂–∞—Ç –Ω–∞ —Å–Ω–µ–∂–Ω–æ–º —Ä–µ–ª—å–µ—Ñ–µ'''

custom_css = """
.center-text {
    text-align: center;
    display: block; /* Ensures the text container behaves like a block element */
}
"""
# Create Gradio interface
with gr.Blocks(title="RuWordNet Taxonomy Prediction Client", theme=gr.themes.Soft(), css=custom_css) as demo:
    gr.Markdown("# üîç RuWordNet Taxonomy Prediction Client")
    gr.Markdown("""
    –≠—Ç–æ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ API —Å–µ—Ä–≤–∏—Å—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Å—Ç–∞ –ø–æ–Ω—è—Ç–∏—è –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ RuWordNet.
    
    **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:**
    1. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    2. –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: "–†—É—á–Ω–æ–π –≤–≤–æ–¥" –∏–ª–∏ "–î–∞—Ç–∞—Å–µ—Ç"
    3. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã
    4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑
    """)

    # Parameters section (shared)
    with gr.Accordion("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã", open=False):
        with gr.Row():
            # Model parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏**")
                max_iterations = gr.Slider(
                    minimum=5,
                    maximum=100,
                    value=50,
                    step=1,
                    label="–ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π"
                )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.5,
                    step=0.1,
                    label="Temperature"
                )
                top_p = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.95,
                    step=0.05,
                    label="Top P"
                )
            
            # Pipeline parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞**")
                with gr.Row():
                    reranking = gr.Checkbox(
                        label="üîÑ –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ",
                        value=True
                    )
                    interpreting = gr.Checkbox(
                        label="üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
                        value=True,
                        interactive=True
                    )
                functions = gr.CheckboxGroup(
                    label="üîß –§—É–Ω–∫—Ü–∏–∏",
                    choices=[
                        ("–ü–æ–ª—É—á–∏—Ç—å –≥–∏–ø–æ–Ω–∏–º—ã", "get_hyponyms"),
                        ("–ü–æ–ª—É—á–∏—Ç—å –≥–∏–ø–µ—Ä–æ–Ω–∏–º—ã", "get_hypernyms")
                    ],
                    value=["get_hyponyms"]
                )
                num_processes = gr.Slider(
                    minimum=1,
                    maximum=3,
                    value=3,
                    step=1,
                    label="üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤",
                    info="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞"
                )
                max_n_starting_nodes = gr.Slider(
                    minimum=0,
                    maximum=3,
                    value=0,
                    step=1,
                    label="üìç –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤",
                    info="0 = —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º, 1-3 = —Ä–µ–∂–∏–º —Å –ø—Ä–µ–¥–∑–∞–¥–∞–Ω–Ω—ã–º–∏ —É–∑–ª–∞–º–∏",
                    interactive=False
                )
    
    # Mode selection
    with gr.Tab("üñäÔ∏è –†—É—á–Ω–æ–π –≤–≤–æ–¥"):
        # Text input
        text_input = gr.Textbox(
            label="üìù –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å —Ç–µ–≥–∞–º–∏ <predict_kb>...</predict_kb>",
            lines=10,
            value=example_text
        )
        
        # Output file for tracking
        manual_output_file = gr.Textbox(
            label="üíæ –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            placeholder="tracking_results/manual_analysis.json",
            info="–ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )

        manual_run_btn = gr.Button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ (3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞)", variant="primary", size="lg")
        
    
    with gr.Tab("üìä –†–µ–∂–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞"):
        with gr.Row():
            dataset_file = gr.File(
                label="üìÅ –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (TSV)",
                file_types=[".tsv"]
            )
            corpus_folder = gr.Textbox(
                label="üìÇ –ü–∞–ø–∫–∞ —Å –∫–æ—Ä–ø—É—Å–æ–º —Ç–µ–∫—Å—Ç–æ–≤",
                value="C:/Users/Admin/Documents/Thesis/corpus/annotated_texts",
                interactive=True
            )
        
        # –ü–∞–ø–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤
        start_nodes_folder = gr.Textbox(
            label="üìÅ –ü–∞–ø–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ (JSON)",
            value="examples/yandex-gpt5_candidates.json",
            interactive=True
        )
        
        start_nodes_info = gr.Textbox(
            label="‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–∞—Ö",
            interactive=False
        )
        
        dataset_info = gr.Textbox(
            label="‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ",
            interactive=False
        )
        
        with gr.Row():
            with gr.Column(scale=3):
                word_dropdown = gr.Dropdown(
                    label="üéØ –ü–æ–∏—Å–∫ –∏ –≤—ã–±–æ—Ä —Å–ª–æ–≤–∞",
                    choices=[],
                    interactive=True,
                    allow_custom_value=True,
                    info="–ù–∞—á–Ω–∏—Ç–µ –ø–µ—á–∞—Ç–∞—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
                )
            with gr.Column(scale=1):
                word_validation = gr.Textbox(
                    label="‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–≤–∞",
                    interactive=False,
                    lines=2
                )
        
        sample_start = gr.Slider(
            minimum=1,
            maximum=1,
            value=1,
            step=1,
            label="üî¢ –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å (–¥–ª—è –±–∞—Ç—á-—Ä–µ–∂–∏–º–∞)",
            interactive=False
        )
        num_samples = gr.Slider(
            minimum=1,
            maximum=1,
            value=1,
            step=1,
            label="üî¢ –ß–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è –±–∞—Ç—á-—Ä–µ–∂–∏–º–∞)",
            interactive=False
        )
        dataset_size = gr.State(1)
        
        # –¢–µ–∫—Å—Ç –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        word_text_display = gr.Textbox(
            label="üìù –¢–µ–∫—Å—Ç –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞",
            lines=8,
            interactive=False
        )

        with gr.Row():
            prev_text_btn = gr.Button("‚Üê", variant="secondary", interactive=False)
            current_text_label = gr.Markdown(value='0', elem_classes="center-text")
            next_text_btn = gr.Button("‚Üí", variant="secondary", interactive=False)
        
        # Output file for dataset tracking
        dataset_output_file = gr.Textbox(
            label="üíæ –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            placeholder="tracking_results/dataset_analysis.json",
            info="–ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )
        
        
        # –ë–∞—Ç—á-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        with gr.Accordion("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏", open=False):
            batch_size = gr.Slider(
                minimum=1,
                maximum=10,
                value=3,
                step=1,
                label="üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞",
                interactive=True
            )
            parallel_mode = gr.Radio(
                choices=["–ø–æ —Ç–µ–∫—Å—Ç–∞–º", "–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"],
                label="üîò –†–µ–∂–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞",
                value="–ø–æ —Ç–µ–∫—Å—Ç–∞–º",
                interactive=True
            )

        with gr.Row():
            dataset_run_btn = gr.Button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ", variant="primary")
            batch_run_btn = gr.Button("üîÑ –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞", variant="secondary")
        
        batch_results = gr.Textbox(
            label="üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏",
            lines=5,
            interactive=False
        )
    
    # 3 parallel outputs
    gr.Markdown("### üìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("#### üîµ –ü—Ä–æ—Ü–µ—Å—Å #1")
            process_output_1 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #1",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_1 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #1",
                lines=5,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü¢ –ü—Ä–æ—Ü–µ—Å—Å #2")
            process_output_2 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #2",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_2 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #2",
                lines=5,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü° –ü—Ä–æ—Ü–µ—Å—Å #3")
            process_output_3 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #3",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_3 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #3",
                lines=5,
                interactive=False
            )
    
    # Examples
    gr.Markdown("### üìù –ü—Ä–∏–º–µ—Ä—ã")
    gr.Examples(
        examples=[
            [example_text, 50, 0.5, 0.95, False, ["get_hyponyms"]],
            ["–≠—Ç–æ—Ç <predict_kb>–≤–µ–ª–æ—Å–∏–ø–µ–¥</predict_kb> –±—ã–ª –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω –≤ –ì–µ—Ä–º–∞–Ω–∏–∏.", 50, 0.5, 0.95, True, ["get_hyponyms", "get_hypernyms"]],
            ["–ù–æ–≤—ã–π <predict_kb>—Å–º–∞—Ä—Ç—Ñ–æ–Ω</predict_kb> –∏–º–µ–µ—Ç –æ—Ç–ª–∏—á–Ω—É—é –∫–∞–º–µ—Ä—É.", 50, 0.5, 0.95, False, ["get_hyponyms"]],
        ],
        inputs=[text_input, max_iterations, temperature, top_p, reranking, functions],
        label="–ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"
    )
    
    # Info
    gr.Markdown("""
    ---
    ### ‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    
    **–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
    1. **–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ**
        - üìä –í—Å–µ –≤—ã–±–∏—Ä–∞–µ–º—ã–µ —Å–∏–Ω—Å–µ—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
        - üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ JSON —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ `tracking_results/`
        - üìà –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —É–∑–ª–∞–º
        - üîç –ö–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å node_id –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –≤ –∏—Å—Ç–æ—Ä–∏—é
    2. **–†–µ–∂–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞**
        - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ü–µ–ª–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏ –ø—É—Ç—è–º–∏ –¥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫–æ—Ä–ø—É—Å–∞
        - –ü–æ–¥–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ —É–∑–ª—ã –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ –∏–∑ –Ω–∞—á–∞–ª–∞ —Å–ø–∏—Å–∫–∞
        - –î–≤–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞
        - –ü—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ª–æ–≤–∞: –≤—ã–±–æ—Ä —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ
        - –í –±–∞—Ç—á-—Ä–µ–∂–∏–º–µ: –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–∏–º–µ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏ —Ç–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        - –¢—Ä–∏ —Ç–∏–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞: –ø–æ —Ç–µ–∫—Å—Ç–∞–º (1 —Ç–µ–∫—Å—Ç = 1 –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞), –ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º (–µ—Å–ª–∏ –æ–Ω–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã) –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (—Å—ç–º–ø–ª–∏–Ω–≥)
    
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞:**
    - **–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ** - —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —á–∏—Å–ª–∞ —Å–∏–Ω—Å–µ—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ —Ü–µ–ª–µ–≤–æ–º—É —Å–ª–æ–≤—É
    - **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑** - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    - **–§—É–Ω–∫—Ü–∏–∏** - –≤—ã–±–µ—Ä–∏—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (get_hyponyms, get_hypernyms)
    
    **–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
    - `not_found` - –ø–æ–Ω—è—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
    - `include in {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∏–Ω–æ–Ω–∏–º–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–∏–Ω—Å–µ—Ç–∞
    - `hyponym of {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è –≥–∏–ø–æ–Ω–∏–º–æ–º (–±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ç–∏–ø–æ–º) —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–Ω—è—Ç–∏—è
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
    - –ó–∞–ø—É—Å—Ç–∏—Ç–µ API: `python api.py`
    """)
    
    # Function to run 3 parallel processes using threads
    def run_parallel_analysis(target_word, texts, max_iterations, temperature, top_p, reranking, interpreting, functions, 
                         output_file, num_processes, start_nodes_path, max_n_starting_nodes, parallel_mode):
        """Run analysis with configurable number of processes"""
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ —É–∑–ª—ã –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ > 0
        start_nodes_dict = {}
        if start_nodes_path and max_n_starting_nodes > 0:
            start_nodes_dict = load_start_nodes(start_nodes_path)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        if not target_word:
            match = re.search(r'<predict_kb>(.*?)</predict_kb>', texts[0])
            target_word = match.group(1).strip() if match else None
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ —É–∑–ª—ã –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        word_start_nodes = []
        if target_word and target_word in start_nodes_dict and max_n_starting_nodes > 0:
            word_start_nodes = start_nodes_dict[target_word][:max_n_starting_nodes]
        elif max_n_starting_nodes > 0:
            logger.warning(f'Word {target_word} not found in start nodes list ({list(start_nodes_dict.keys())[0]},...)')
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã/—É–∑–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        process_data = []
        for i in range(num_processes):
            if parallel_mode == '–ø–æ —Ç–µ–∫—Å—Ç–∞–º' and i < len(texts):
                text = texts[i]
            else:
                text = texts[0]
            if word_start_nodes:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∑–∞–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª
                if parallel_mode == '–ø–æ —Å—Ç–∞—Ä—Ç–æ–≤—ã–º —É–∑–ª–∞–º' and i < len(word_start_nodes):
                    start_node = word_start_nodes[i]
                else:
                    start_node = word_start_nodes[0]
                process_data.append({
                    'text': text,
                    'start_node_id': start_node,
                    'process_id': i + 1
                })
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ –ø—Ä–µ–¥–∑–∞–¥–∞–Ω–Ω–æ–≥–æ —É–∑–ª–∞
                process_data.append({
                    'text': text,
                    'process_id': i + 1
                })
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        output_files = [None] * num_processes
        timestamp = int(time.time())
        if output_file:
            base_name = output_file.rsplit('.', 1)[0] if '.' in output_file else output_file
            extension = output_file.rsplit('.', 1)[1] if '.' in output_file else 'json'
            for i in range(num_processes):
                output_files[i] = f"{base_name}_process{i+1}_{timestamp}.{extension}"
        else:
            for i in range(num_processes):
                output_files[i] = f"tracking_results/single_word_process{i+1}_{timestamp}.json"
        
        # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        queues = [Queue() for _ in range(num_processes)]
        
        def run_stream_with_start_node(queue_idx, proc_data, max_iterations, temperature, top_p, 
                                    reranking, interpreting, functions, output_file):
            """Run streaming in a thread with optional start node"""
            start_node_id = proc_data.get('start_node_id')
            request_args = [proc_data['text'], max_iterations, temperature, top_p, reranking, interpreting, functions, output_file]
            if start_node_id:
                request_args.append(start_node_id)
            try:
                for process_log, final_result in process_text_stream(*request_args):
                    queues[queue_idx].put((process_log, final_result))
            except Exception as e:
                queues[queue_idx].put((f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ #{proc_data['process_id']}: {str(e)}", ""))
            finally:
                queues[queue_idx].put(None)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏
        threads = []
        for i in range(num_processes):
            thread = threading.Thread(
                target=run_stream_with_start_node,
                args=(i, process_data[i], max_iterations, temperature, top_p, reranking, interpreting, functions, output_files[i])
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö 3 –≤—ã—Ö–æ–¥–æ–≤
        results = [
            (f"üîµ –ü—Ä–æ—Ü–µ—Å—Å #1 {'(—Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª: ' + process_data[0].get('start_node_id', '–Ω–µ—Ç') + ')' if len(process_data) > 0 else ''}", "") if num_processes >= 1 else ("–ü—Ä–æ—Ü–µ—Å—Å –æ—Ç–∫–ª—é—á–µ–Ω", ""),
            (f"üü¢ –ü—Ä–æ—Ü–µ—Å—Å #2 {'(—Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª: ' + process_data[1].get('start_node_id', '–Ω–µ—Ç') + ')' if len(process_data) > 1 else ''}", "") if num_processes >= 2 else ("–ü—Ä–æ—Ü–µ—Å—Å –æ—Ç–∫–ª—é—á–µ–Ω", ""),
            (f"üü° –ü—Ä–æ—Ü–µ—Å—Å #3 {'(—Å—Ç–∞—Ä—Ç–æ–≤—ã–π —É–∑–µ–ª: ' + process_data[2].get('start_node_id', '–Ω–µ—Ç') + ')' if len(process_data) > 2 else ''}", "") if num_processes >= 3 else ("–ü—Ä–æ—Ü–µ—Å—Å –æ—Ç–∫–ª—é—á–µ–Ω", "")
        ]
        
        active = [i < num_processes for i in range(3)]
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –æ—á–µ—Ä–µ–¥–µ–π
        while any(active[:num_processes]):
            for i in range(num_processes):
                if active[i]:
                    try:
                        while not queues[i].empty():
                            item = queues[i].get_nowait()
                            if item is None:
                                active[i] = False
                            else:
                                results[i] = item
                    except:
                        pass
            
            yield (results[0][0], results[0][1],
                results[1][0], results[1][1],
                results[2][0], results[2][1])
            
            time.sleep(0.05)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        yield (results[0][0], results[0][1],
            results[1][0], results[1][1],
            results[2][0], results[2][1])
        
    def run_manual_parallel_analysis(*args):
        if not isinstance(args[1], list):
            args[1] = [args[1]]
        return run_parallel_analysis(*args)

    # Event handlers
    manual_run_btn.click(
        fn=run_manual_parallel_analysis,
        inputs=[word_dropdown, text_input, max_iterations, temperature, top_p, reranking, interpreting, functions, 
                manual_output_file, num_processes, start_nodes_folder, max_n_starting_nodes],
        outputs=[process_output_1, result_output_1,
                process_output_2, result_output_2,
                process_output_3, result_output_3]
    )
    
    start_nodes_folder.change(
        fn=get_start_nodes_info,
        inputs=[start_nodes_folder],
        outputs=[start_nodes_info, max_n_starting_nodes, parallel_mode]
    )
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–∞–π–¥–µ—Ä–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
    def safe_get_dataset_info(file):
        if not file:
            return "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω", gr.update(choices=[]), 1, 1, 1
        
        file_path = file.name if hasattr(file, 'name') else str(file)
        return get_dataset_info(file_path)

    dataset_file.change(
        fn=safe_get_dataset_info,
        inputs=[dataset_file],
        outputs=[dataset_info, word_dropdown, sample_start, num_samples, dataset_size]
    )
    sample_start.change(
        fn=lambda x, y: gr.update(maximum=y-x+1, value=y-x+1, interactive=True),
        inputs=[sample_start, dataset_size],
        outputs=[num_samples]
    )
    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ª–æ–≤–∞
    word_dropdown.change(
        fn=lambda dataset_path, corpus, word: load_word_text_from_corpus(dataset_path, corpus, word),
        inputs=[dataset_file, corpus_folder, word_dropdown],
        outputs=[word_text_display, prev_text_btn, current_text_label, next_text_btn]
    )
    # –í—ã–±–æ—Ä —Ç–µ–∫—Å—Ç–∞
    prev_text_btn.click(
        fn=lambda dataset_path, corpus, word, index: load_word_text_from_corpus(dataset_path, corpus, word, int(index) - 2),
        inputs=[dataset_file, corpus_folder, word_dropdown, current_text_label],
        outputs=[word_text_display, prev_text_btn, current_text_label, next_text_btn]
    )
    next_text_btn.click(
        fn=lambda dataset_path, corpus, word, index: load_word_text_from_corpus(dataset_path, corpus, word, int(index)),
        inputs=[dataset_file, corpus_folder, word_dropdown, current_text_label],
        outputs=[word_text_display, prev_text_btn, current_text_label, next_text_btn]
    )
    reranking.change(
        fn=lambda x: gr.update(interactive=False) if not x else gr.update(interactive=True),
        inputs=[reranking],
        outputs=[interpreting]
    )
    # –†–µ–∂–∏–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    dataset_run_btn.click(
        fn=process_dataset_item,
        inputs=[
            dataset_file, corpus_folder, word_dropdown, max_iterations,
            temperature, top_p, reranking, interpreting, functions,
            num_processes, start_nodes_folder, max_n_starting_nodes,
            parallel_mode, dataset_output_file
        ],
        outputs=[process_output_1, result_output_1,
                process_output_2, result_output_2,
                process_output_3, result_output_3]
    )
    word_dropdown.change(
        fn=lambda dataset_file, query: search_words_in_dataset(dataset_file.name if dataset_file else "", query) if query else query.upper(),
        inputs=[dataset_file, word_dropdown],
        outputs=[word_dropdown]
    )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    word_dropdown.select(
        fn=lambda dataset_file, word: validate_word_in_dataset(dataset_file.name if dataset_file else "", word),
        inputs=[dataset_file, word_dropdown],
        outputs=[word_validation]
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–Ω–æ–º –≤—ã–±–æ—Ä–µ —Å–ª–æ–≤–∞
    word_dropdown.select(
        fn=lambda dataset_path, corpus, word: load_word_text_from_corpus(dataset_path, corpus, word),
        inputs=[dataset_file, corpus_folder, word_dropdown],
        outputs=[word_text_display, prev_text_btn, current_text_label, next_text_btn]
    )
    
    # –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º max_samples
    batch_run_btn.click(
        fn=process_dataset_batch,
        inputs=[dataset_file, corpus_folder, sample_start, num_samples, num_processes, batch_size, max_iterations, 
                temperature, top_p, reranking, interpreting, functions, start_nodes_folder, max_n_starting_nodes, parallel_mode],
        outputs=[batch_results]
    )
    

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=5003, share=False)