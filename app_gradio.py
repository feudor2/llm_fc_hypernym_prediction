import gradio as gr
import requests
import json
from typing import Generator
import threading
from queue import Queue
import time
import os
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
from data_processing import load_dataset, load_corpus_text


def process_text_stream(text: str, max_iterations: int, temperature: float, top_p: float, 
                       reranking: bool, functions: list):
    """Process text using the streaming API endpoint"""
    
    # Validate input
    if '<predict_kb>' not in text or '</predict_kb>' not in text:
        yield "‚ùå –û—à–∏–±–∫–∞: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–≥–∏ <predict_kb>...</predict_kb>", ""
        return
    
    # API URL (hardcoded)
    api_url = "http://localhost:8500"
    
    # Prepare request with pipeline parameters
    endpoint = f"{api_url.rstrip('/')}/predict/stream"
    payload = {
        "text": text,
        "max_iterations": max_iterations,
        "temperature": temperature,
        "top_p": top_p,
        "reranking": reranking,
        "functions": functions
    }
    
    process_log = ""
    final_result = ""
    
    try:
        # Make streaming request
        with requests.post(endpoint, json=payload, stream=True, timeout=300) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    
                    # Parse SSE format
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # Remove 'data: ' prefix
                        
                        try:
                            data = json.loads(data_str)
                            event_type = data.get('type')
                            
                            if event_type == 'iteration':
                                iteration_info = f"\n{'='*54}\nüîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {data['iteration']}\n{'='*54}\n\n"
                                process_log += iteration_info
                                yield process_log, final_result
                            
                            elif event_type == 'thought':
                                thought = f"üí≠ –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:\n{data['content']}\n\n"
                                process_log += thought
                                yield process_log, final_result
                            
                            elif event_type == 'tool_call':
                                tool_info = f"üîß –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: {data['function']}\n"
                                tool_info += f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã: {json.dumps(data['args'], ensure_ascii=False)}\n"
                                tool_info += f"–£–∑–µ–ª: {data['node_name']}\n\n"
                                process_log += tool_info
                                yield process_log, final_result
                            
                            elif event_type == 'tool_response':
                                # Display the markdown formatted function response
                                response_info = f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏:\n{data['content']}\n"
                                process_log += response_info
                                yield process_log, final_result
                            
                            elif event_type == 'final':
                                final_result = f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n\n{data['result']}"
                                process_log += f"\n{final_result}\n"
                                process_log += f"\n{'='*54}\n‚úîÔ∏è –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω\n{'='*54}\n"
                                yield process_log, final_result
                                return
                            
                            elif event_type == 'error':
                                error_msg = f"‚ùå –û—à–∏–±–∫–∞: {data['message']}\n"
                                process_log += error_msg
                                yield process_log, final_result
                                return
                        
                        except json.JSONDecodeError:
                            continue
        
    except requests.exceptions.Timeout:
        yield "‚ùå –û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞", ""
    except requests.exceptions.ConnectionError:
        yield f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ API –∑–∞–ø—É—â–µ–Ω –Ω–∞ {api_url}", ""
    except requests.exceptions.HTTPError as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ HTTP: {e.response.status_code} - {e.response.text}", ""
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞: {str(e)}", ""


def process_dataset_item(dataset_path: str, corpus_folder: str, word: str, 
                        max_iterations: int, temperature: float, top_p: float, 
                        reranking: bool, functions: list):
    """Process a specific word from dataset using corpus text"""
    try:
        # Load corpus text for the word
        text = load_corpus_text(corpus_folder, word)
        if not text:
            yield f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–≤–∞: {word}", ""
            return
        
        # Process using the streaming function
        for process_log, final_result in process_text_stream(
            text, max_iterations, temperature, top_p, reranking, functions
        ):
            yield process_log, final_result
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–≤–∞ '{word}': {str(e)}", ""


def get_dataset_info(dataset_path: str):
    """Get information about loaded dataset"""
    if not dataset_path or not os.path.exists(dataset_path):
        return "–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –≤—ã–±—Ä–∞–Ω –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω", [], ""
    
    try:
        dataset = load_dataset(dataset_path)
        words = list(dataset.keys())
        info = f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(words)} —Å–ª–æ–≤"
        return info, words, words[0] if words else ""
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}", [], ""


# Example text
example_text = '''–ö–∞–∂–¥–æ–µ –ª–µ—Ç–æ –≥—Ä—É–ø–ø—ã —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Å–µ–±—è –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ –ø–æ–∏—Å–∫–∏ —Å–Ω–µ–≥–∞ –∏ –ª—å–¥–∞. –ß–∞—â–µ –≤—Å–µ–≥–æ –∏—Ö –Ω–∞–∑—ã–≤–∞—é—Ç –∞–ª—å–ø–∏–Ω–∏—Å—Ç—ã, –∏ –æ–Ω–∏ –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è –≥–æ–¥–∞ –Ω–µ –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ—Å–µ—á—å –ª–µ–¥–Ω–∏–∫ –∏–ª–∏ —Ç—Ä–æ–ø–∏—Ç—å –ø–æ —Å–Ω–µ–≥—É –¥–æ –≤–µ—Ä—à–∏–Ω—ã. –•—Ä–∞–±—Ä—ã–µ –ø—Ä–æ—Ñ–∏ –¥–∞–∂–µ –≥–æ—Ç–æ–≤—ã –ª–µ–∑—Ç—å –ø–æ —Å–∫–∞–ª–∞–º —Å–æ –ª—å–¥–æ–º, –≤—ã–±–∏—Ä–∞—è –∑–∞–ø—Ä–µ–¥–µ–ª—å–Ω–æ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã. –ì–æ—Ä–Ω—ã–µ —Ç—É—Ä–∏—Å—Ç—ã —Ç–æ–∂–µ —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –≥—É–ª—è—é—Ç —Å—Ä–µ–¥–∏ –≤–µ—á–Ω–æ–π –º–µ—Ä–∑–ª–æ—Ç—ã –Ω–∞ –≤—ã—Å–æ—Ç–∞—Ö –±–æ–ª–µ–µ 4000 –º–µ—Ç—Ä–æ–≤ –Ω–∞–¥ —É—Ä–æ–≤–Ω–µ–º –º–æ—Ä—è. –ò –≤—Å–µ–º –∏–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ–µ —Å—Ü–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ —Å–∫–æ–ª—å–∑–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª—å–¥–∞.

–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π –∫—É–∑–Ω–µ—Ü –∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–æ–π –∞–ª—å–ø–∏–Ω–∏—Å—Ç—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ì–µ–Ω—Ä–∏ –ì—Ä–∏–≤–µ–ª—å –±–æ–ª–µ–µ 100 –ª–µ—Ç –Ω–∞–∑–∞–¥ —Å–Ω–∞–±–¥–∏–ª –æ–¥–Ω–∏—Ö –∏–∑ –ø–µ—Ä–≤—ã—Ö –≤–æ—Å—Ö–æ–¥–∏—Ç–µ–ª–µ–π –ø—Ä–æ–æ–±—Ä–∞–∑–æ–º —Ç–æ–≥–æ, —á—Ç–æ —Å–µ–π—á–∞—Å –Ω–∞–∑—ã–≤–∞—é—Ç <predict_kb>–∫–æ—à–∫–∞–º–∏</predict_kb>. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –±—ã–ª–∏ –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ä—è–¥ —Å–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Å–∫–æ–± —Å –∑–∞–æ—Å—Ç—Ä—ë–Ω–Ω—ã–º–∏ —à–∏–ø–∞–º–∏ –∏ —Ä–µ–º–Ω—è–º–∏ –¥–ª—è –∫—Ä–µ–ø–ª–µ–Ω–∏—è. –û–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ —Ç–∞–∫—Ç–∏–∫—É –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Å–Ω–µ–∂–Ω–æ-–ª–µ–¥–æ–≤–æ–º—É —Å–∫–ª–æ–Ω—É –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–æ–≤.

–° —Ç–µ—Ö –≤—Ä–µ–º—ë–Ω –º–æ–¥–µ–ª–∏ –∑–∞–º–µ—Ç–Ω–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–ª–∏, –Ω–æ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —ç—Ç–æ –∏–∑–¥–µ–ª–∏—è –∏–∑ –º–µ—Ç–∞–ª–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–ø—è—Ç—Å—è –∫ –±–æ—Ç–∏–Ω–∫–∞–º, –≤–≥—Ä—ã–∑–∞—é—Ç—Å—è –≤ –ª—ë–¥ –∏ –¥–µ—Ä–∂–∞—Ç –Ω–∞ —Å–Ω–µ–∂–Ω–æ–º —Ä–µ–ª—å–µ—Ñ–µ'''


# Create Gradio interface
with gr.Blocks(title="RuWordNet Taxonomy Prediction Client", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üîç RuWordNet Taxonomy Prediction Client")
    gr.Markdown("""
    –≠—Ç–æ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ API —Å–µ—Ä–≤–∏—Å—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Å—Ç–∞ –ø–æ–Ω—è—Ç–∏—è –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ RuWordNet.
    
    **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:**
    1. –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: "–†—É—á–Ω–æ–π –≤–≤–æ–¥" –∏–ª–∏ "–î–∞—Ç–∞—Å–µ—Ç"
    2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    3. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã
    4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑
    """)
    
    # Mode selection
    with gr.Tab("üñäÔ∏è –†—É—á–Ω–æ–π –≤–≤–æ–¥"):
        # Text input
        text_input = gr.Textbox(
            label="üìù –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å —Ç–µ–≥–∞–º–∏ <predict_kb>...</predict_kb>",
            lines=10,
            value=example_text
        )
        
        manual_run_btn = gr.Button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ (3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞)", variant="primary", size="lg")
    
    with gr.Tab("üìä –†–µ–∂–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞"):
        with gr.Row():
            dataset_file = gr.File(
                label="üìÅ –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (TSV)",
                file_types=[".tsv"]
            )
            corpus_folder = gr.Textbox(
                label="üìÇ –ü–∞–ø–∫–∞ —Å –∫–æ—Ä–ø—É—Å–æ–º —Ç–µ–∫—Å—Ç–æ–≤",
                placeholder="–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏",
                interactive=True
            )
        
        # Browse buttons for file explorer access
        with gr.Row():
            browse_dataset_btn = gr.Button("üîç –û–±–∑–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞", size="sm")
            browse_corpus_btn = gr.Button("üîç –û–±–∑–æ—Ä –∫–æ—Ä–ø—É—Å–∞", size="sm")
        
        dataset_info = gr.Textbox(
            label="‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ",
            interactive=False
        )
        
        with gr.Row():
            word_dropdown = gr.Dropdown(
                label="üéØ –í—ã–±–µ—Ä–∏—Ç–µ —Å–ª–æ–≤–æ",
                choices=[],
                interactive=True
            )
            num_samples = gr.Slider(
                minimum=1,
                maximum=100,
                value=1,
                step=1,
                label="üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤"
            )
        
        dataset_run_btn = gr.Button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ", variant="primary", size="lg")
    
    # Parameters section (shared)
    with gr.Accordion("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã", open=True):
        with gr.Row():
            # Model parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏**")
                max_iterations = gr.Slider(
                    minimum=5,
                    maximum=100,
                    value=50,
                    step=1,
                    label="–ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π"
                )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.5,
                    step=0.1,
                    label="Temperature"
                )
                top_p = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.95,
                    step=0.05,
                    label="Top P"
                )
            
            # Pipeline parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞**")
                reranking = gr.Checkbox(
                    label="üîÑ –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    value=False
                )
                functions = gr.CheckboxGroup(
                    label="üîß –§—É–Ω–∫—Ü–∏–∏",
                    choices=[
                        ("get_hyponyms", "get_hyponyms"),
                        ("get_hypernyms", "get_hypernyms")
                    ],
                    value=["get_hyponyms"]
                )
    
    # 3 parallel outputs
    gr.Markdown("### üìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("#### üîµ –ü—Ä–æ—Ü–µ—Å—Å #1")
            process_output_1 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #1",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_1 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #1",
                lines=3,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü¢ –ü—Ä–æ—Ü–µ—Å—Å #2")
            process_output_2 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #2",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_2 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #2",
                lines=3,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü° –ü—Ä–æ—Ü–µ—Å—Å #3")
            process_output_3 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #3",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_3 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #3",
                lines=3,
                interactive=False
            )
    
    # Examples
    gr.Markdown("### üìù –ü—Ä–∏–º–µ—Ä—ã")
    gr.Examples(
        examples=[
            [example_text, 50, 0.5, 0.95, False, ["get_hyponyms"]],
            ["–≠—Ç–æ—Ç <predict_kb>–≤–µ–ª–æ—Å–∏–ø–µ–¥</predict_kb> –±—ã–ª –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω –≤ –ì–µ—Ä–º–∞–Ω–∏–∏.", 50, 0.5, 0.95, True, ["get_hyponyms", "get_hypernyms"]],
            ["–ù–æ–≤—ã–π <predict_kb>—Å–º–∞—Ä—Ç—Ñ–æ–Ω</predict_kb> –∏–º–µ–µ—Ç –æ—Ç–ª–∏—á–Ω—É—é –∫–∞–º–µ—Ä—É.", 50, 0.5, 0.95, False, ["get_hyponyms"]],
        ],
        inputs=[text_input, max_iterations, temperature, top_p, reranking, functions],
        label="–ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"
    )
    
    # Info
    gr.Markdown("""
    ---
    ### ‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞:**
    - **–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ** - –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - **–§—É–Ω–∫—Ü–∏–∏** - –≤—ã–±–µ—Ä–∏—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (get_hyponyms, get_hypernyms)
    
    **–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
    - `not_found` - –ø–æ–Ω—è—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
    - `include in {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∏–Ω–æ–Ω–∏–º–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–∏–Ω—Å–µ—Ç–∞
    - `hyponym of {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è –≥–∏–ø–æ–Ω–∏–º–æ–º (–±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ç–∏–ø–æ–º) —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–Ω—è—Ç–∏—è
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
    - –ó–∞–ø—É—Å—Ç–∏—Ç–µ API: `python api.py`
    """)
    
    # Function to run 3 parallel processes using threads
    def run_parallel_analysis(text, max_iterations, temperature, top_p, reranking, functions):
        """Run 3 parallel analysis processes using threads"""
        
        # Queues to communicate between threads
        queues = [Queue(), Queue(), Queue()]
        
        def run_stream(queue_idx, text, max_iterations, temperature, top_p, reranking, functions):
            """Run streaming in a thread and put results in queue"""
            try:
                for process_log, final_result in process_text_stream(
                    text, max_iterations, temperature, top_p, reranking, functions
                ):
                    queues[queue_idx].put((process_log, final_result))
            except Exception as e:
                queues[queue_idx].put((f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ: {str(e)}", ""))
            finally:
                # Signal completion
                queues[queue_idx].put(None)
        
        # Start 3 threads
        threads = []
        for i in range(3):
            thread = threading.Thread(
                target=run_stream,
                args=(i, text, max_iterations, temperature, top_p, reranking, functions)
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # Track state for each process
        active = [True, True, True]
        results = [("üîµ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #1...", ""), ("üü¢ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #2...", ""), ("üü° –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #3...", "")]
        
        # Continuously check queues and yield updates
        while any(active):
            updated = False
            
            for i in range(3):
                if active[i]:
                    try:
                        # Try to get item without blocking
                        while not queues[i].empty():
                            item = queues[i].get_nowait()
                            if item is None:
                                active[i] = False
                            else:
                                results[i] = item
                                updated = True
                    except:
                        pass
            
            # Yield current state (even if not updated to keep UI responsive)
            yield (results[0][0], results[0][1],
                   results[1][0], results[1][1],
                   results[2][0], results[2][1])
            
            # Small sleep to prevent busy waiting but keep responsive
            time.sleep(0.05)
        
        # Final yield to ensure all results are shown
        yield (results[0][0], results[0][1],
               results[1][0], results[1][1],
               results[2][0], results[2][1])
    
    # Event handlers
    manual_run_btn.click(
        fn=run_parallel_analysis,
        inputs=[text_input, max_iterations, temperature, top_p, reranking, functions],
        outputs=[process_output_1, result_output_1,
                 process_output_2, result_output_2,
                 process_output_3, result_output_3]
    )
    
    dataset_run_btn.click(
        fn=lambda dataset_file, corpus_folder, word, max_iter, temp, top_p_val, rerank, funcs: 
            list(process_dataset_item(
                dataset_file.name if dataset_file else "", 
                corpus_folder, word, max_iter, temp, top_p_val, rerank, funcs
            ))[-1] if dataset_file and corpus_folder and word else ("‚ùå –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è", ""),
        inputs=[dataset_file, corpus_folder, word_dropdown, max_iterations, temperature, top_p, reranking, functions],
        outputs=[process_output_1, result_output_1]
    )
    
    # Update dataset info when file is uploaded
    dataset_file.change(
        fn=lambda file: get_dataset_info(file.name if file else ""),
        inputs=[dataset_file],
        outputs=[dataset_info, word_dropdown, word_dropdown]
    )


if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=5003, share=False)