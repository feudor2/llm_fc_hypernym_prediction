import gradio as gr
import requests
import json
from typing import Generator
import threading
from queue import Queue
import time
import os
import glob
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
from data_processing import load_dataset, load_corpus_text

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_text_stream(text: str, max_iterations: int, temperature: float, top_p: float, 
                       reranking: bool, functions: list):
    """Process text using the streaming API endpoint"""
    
    # Validate input
    if '<predict_kb>' not in text or '</predict_kb>' not in text:
        yield "‚ùå –û—à–∏–±–∫–∞: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–≥–∏ <predict_kb>...</predict_kb>", ""
        return
    
    # –ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é —Å tools.py
    valid_functions = ["get_hyponyms", "get_hypernyms"]
    functions = [f for f in functions if f in valid_functions]
    
    if not functions:
        functions = ["get_hyponyms"]  # Fallback
    
    # API URL (hardcoded)
    api_url = "http://localhost:8500"
    
    # Prepare request with pipeline parameters
    endpoint = f"{api_url.rstrip('/')}/predict/stream"
    payload = {
        "text": text,
        "max_iterations": max_iterations,
        "temperature": temperature,
        "top_p": top_p,
        "reranking": reranking,
        "functions": functions  # –¢–µ–ø–µ—Ä—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è
    }
    
    process_log = ""
    final_result = ""
    
    try:
        # Make streaming request
        with requests.post(endpoint, json=payload, stream=True, timeout=300) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    
                    # Parse SSE format
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # Remove 'data: ' prefix
                        
                        try:
                            data = json.loads(data_str)
                            event_type = data.get('type')
                            
                            if event_type == 'iteration':
                                iteration_info = f"\n{'='*54}\nüîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {data['iteration']}\n{'='*54}\n\n"
                                process_log += iteration_info
                                yield process_log, final_result
                            
                            elif event_type == 'thought':
                                thought = f"üí≠ –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:\n{data['content']}\n\n"
                                process_log += thought
                                yield process_log, final_result
                            
                            elif event_type == 'tool_call':
                                tool_info = f"üîß –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: {data['function']}\n"
                                tool_info += f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã: {json.dumps(data['args'], ensure_ascii=False)}\n"
                                tool_info += f"–£–∑–µ–ª: {data['node_name']}\n\n"
                                process_log += tool_info
                                yield process_log, final_result
                            
                            elif event_type == 'tool_response':
                                # Display the markdown formatted function response
                                response_info = f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏:\n{data['content']}\n"
                                process_log += response_info
                                yield process_log, final_result
                            
                            elif event_type == 'final':
                                final_result = f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n\n{data['result']}"
                                process_log += f"\n{final_result}\n"
                                process_log += f"\n{'='*54}\n‚úîÔ∏è –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω\n{'='*54}\n"
                                yield process_log, final_result
                                return
                            
                            elif event_type == 'error':
                                error_msg = f"‚ùå –û—à–∏–±–∫–∞: {data['message']}\n"
                                process_log += error_msg
                                yield process_log, final_result
                                return
                        
                        except json.JSONDecodeError:
                            continue
        
    except requests.exceptions.Timeout:
        yield "‚ùå –û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞", ""
    except requests.exceptions.ConnectionError:
        yield f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ API –∑–∞–ø—É—â–µ–Ω –Ω–∞ {api_url}", ""
    except requests.exceptions.HTTPError as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ HTTP: {e.response.status_code} - {e.response.text}", ""
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞: {str(e)}", ""


def process_dataset_item(dataset_path: str, corpus_folder: str, word: str, 
                        max_iterations: int, temperature: float, top_p: float, 
                        reranking: bool, functions: list):
    """Process a specific word from dataset using corpus text"""
    try:
        # Load corpus text for the word
        text = load_corpus_text(corpus_folder, word)
        if not text:
            yield f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–≤–∞: {word}", ""
            return
        
        # Process using the streaming function
        for process_log, final_result in process_text_stream(
            text, max_iterations, temperature, top_p, reranking, functions
        ):
            yield process_log, final_result
    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–≤–∞ '{word}': {str(e)}", ""


def get_dataset_info(dataset_path: str):
    if not dataset_path or not os.path.exists(dataset_path):
        return "–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –≤—ã–±—Ä–∞–Ω –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω", gr.update(choices=[], interactive=False), gr.update(maximum=1, value=1)
    
    try:
        dataset = load_dataset(dataset_path)
        words = list(dataset.keys())
        max_samples = len(words)
        info = f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(words)} —Å–ª–æ–≤"
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ choices –¥–ª—è dropdown, –Ω–æ –¥–µ–ª–∞–µ–º –µ–≥–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º –¥–ª—è –ø–æ–∏—Å–∫–∞
        return (
            info, 
            gr.update(choices=[], interactive=True, allow_custom_value=True),
            gr.update(maximum=max_samples, value=max_samples, interactive=True)
        )
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}", gr.update(choices=[], interactive=False), gr.update(maximum=1, value=1)

def search_words_in_dataset(dataset_path: str, search_query: str):
    """Search for words in dataset that match the query"""
    if not dataset_path or not search_query:
        return gr.update(choices=[])
    
    try:
        dataset = load_dataset(dataset_path)
        words = list(dataset.keys())
        
        # –ü–æ–∏—Å–∫ —Å–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∑–∞–ø—Ä–æ—Å (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
        search_query = search_query.upper().strip()
        matching_words = [word for word in words if search_query in word.upper()]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ 50 —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        matching_words = matching_words[:50]
        
        return gr.update(choices=matching_words)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {e}")
        return gr.update(choices=[])

def validate_word_in_dataset(dataset_path: str, word: str):
    """Validate that the word exists in dataset"""
    if not dataset_path or not word:
        return "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ª–æ–≤–æ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"
    
    try:
        dataset = load_dataset(dataset_path)
        if word in dataset:
            return f"‚úÖ –°–ª–æ–≤–æ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
        else:
            # –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
            words = list(dataset.keys())
            similar = [w for w in words if word.upper() in w.upper() or w.upper() in word.upper()][:5]
            if similar:
                return f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: {', '.join(similar)}"
            else:
                return f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}"

# –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–æ—Ä–ø—É—Å–∞:
def load_word_text_from_corpus(corpus_folder: str, word: str):
    """Load text for specific word from corpus"""
    if not corpus_folder or not word:
        return ""
    
    # –ò—â–µ–º —Ñ–∞–π–ª –°–õ–û–í–û.txt
    file_path = os.path.join(corpus_folder, f"{word}.txt")
    logger.debug(f'–ò—â–µ–º —Ñ–∞–π–ª {file_path}')
    
    if os.path.exists(file_path):
        try:
            text = load_corpus_text(corpus_folder, word)
            logger.debug(f'–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç {text}')
            return text
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}"
    
    return f"‚ùå –§–∞–π–ª {word}.txt –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–ø—É—Å–µ"

# –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏:
def process_dataset_batch(dataset_file, corpus_folder, max_samples, batch_size, max_iterations, 
                         temperature, top_p, reranking, functions, progress=gr.Progress()):
    """Process dataset in batches with max_samples limit"""
    if not dataset_file or not corpus_folder:
        return "‚ùå –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø–∞–ø–∫—É –∫–æ—Ä–ø—É—Å–∞"
    
    try:
        dataset = load_dataset(dataset_file.name)
        all_words = list(dataset.keys())
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º max_samples
        words_to_process = all_words[:max_samples]
        total_words = len(words_to_process)
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {total_words} —Å–ª–æ–≤ –∏–∑ {len(all_words)} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_samples})")
        
        results = {}
        processed_count = 0
        
        for i in range(0, total_words, batch_size):
            batch_words = words_to_process[i:i+batch_size]
            batch_end = min(i + batch_size, total_words)
            
            progress((processed_count)/total_words, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤ {i+1}-{batch_end} –∏–∑ {total_words}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤ –±–∞—Ç—á–µ
            for word in batch_words:
                try:
                    text = load_word_text_from_corpus(corpus_folder, word)
                    if "‚ùå" in text:
                        results[word] = {"error": text}
                        processed_count += 1
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
                    stream_results = list(process_text_stream(
                        text, max_iterations, temperature, top_p, reranking, functions
                    ))
                    if stream_results:
                        final_log, final_result = stream_results[-1]
                        results[word] = {
                            "result": final_result,
                            "log": final_log
                        }
                    else:
                        results[word] = {"error": "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"}
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞ {word}: {e}")
                    results[word] = {"error": str(e)}
                
                processed_count += 1
                progress(processed_count/total_words, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}/{total_words} —Å–ª–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_file = f"test_results/batch_results_{total_words}words.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} —Å–ª–æ–≤ –∏–∑ {len(all_words)}. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}"
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
    
# –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤:
def get_start_nodes_info(start_nodes_folder: str):
    """Get information about start nodes folder"""
    if not start_nodes_folder or not os.path.exists(start_nodes_folder):
        return "–ü–∞–ø–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ –Ω–µ –≤—ã–±—Ä–∞–Ω–∞ –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    
    json_files = glob.glob(os.path.join(start_nodes_folder, "*.json"))
    return f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(json_files)} —Ñ–∞–π–ª–æ–≤ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤"


# Example text
example_text = '''–ö–∞–∂–¥–æ–µ –ª–µ—Ç–æ –≥—Ä—É–ø–ø—ã —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Å–µ–±—è –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ –ø–æ–∏—Å–∫–∏ —Å–Ω–µ–≥–∞ –∏ –ª—å–¥–∞. –ß–∞—â–µ –≤—Å–µ–≥–æ –∏—Ö –Ω–∞–∑—ã–≤–∞—é—Ç –∞–ª—å–ø–∏–Ω–∏—Å—Ç—ã, –∏ –æ–Ω–∏ –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è –≥–æ–¥–∞ –Ω–µ –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ—Å–µ—á—å –ª–µ–¥–Ω–∏–∫ –∏–ª–∏ —Ç—Ä–æ–ø–∏—Ç—å –ø–æ —Å–Ω–µ–≥—É –¥–æ –≤–µ—Ä—à–∏–Ω—ã. –•—Ä–∞–±—Ä—ã–µ –ø—Ä–æ—Ñ–∏ –¥–∞–∂–µ –≥–æ—Ç–æ–≤—ã –ª–µ–∑—Ç—å –ø–æ —Å–∫–∞–ª–∞–º —Å–æ –ª—å–¥–æ–º, –≤—ã–±–∏—Ä–∞—è –∑–∞–ø—Ä–µ–¥–µ–ª—å–Ω–æ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã. –ì–æ—Ä–Ω—ã–µ —Ç—É—Ä–∏—Å—Ç—ã —Ç–æ–∂–µ —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –≥—É–ª—è—é—Ç —Å—Ä–µ–¥–∏ –≤–µ—á–Ω–æ–π –º–µ—Ä–∑–ª–æ—Ç—ã –Ω–∞ –≤—ã—Å–æ—Ç–∞—Ö –±–æ–ª–µ–µ 4000 –º–µ—Ç—Ä–æ–≤ –Ω–∞–¥ —É—Ä–æ–≤–Ω–µ–º –º–æ—Ä—è. –ò –≤—Å–µ–º –∏–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ–µ —Å—Ü–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ —Å–∫–æ–ª—å–∑–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª—å–¥–∞.

–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π –∫—É–∑–Ω–µ—Ü –∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–æ–π –∞–ª—å–ø–∏–Ω–∏—Å—Ç—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ì–µ–Ω—Ä–∏ –ì—Ä–∏–≤–µ–ª—å –±–æ–ª–µ–µ 100 –ª–µ—Ç –Ω–∞–∑–∞–¥ —Å–Ω–∞–±–¥–∏–ª –æ–¥–Ω–∏—Ö –∏–∑ –ø–µ—Ä–≤—ã—Ö –≤–æ—Å—Ö–æ–¥–∏—Ç–µ–ª–µ–π –ø—Ä–æ–æ–±—Ä–∞–∑–æ–º —Ç–æ–≥–æ, —á—Ç–æ —Å–µ–π—á–∞—Å –Ω–∞–∑—ã–≤–∞—é—Ç <predict_kb>–∫–æ—à–∫–∞–º–∏</predict_kb>. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –±—ã–ª–∏ –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ä—è–¥ —Å–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Å–∫–æ–± —Å –∑–∞–æ—Å—Ç—Ä—ë–Ω–Ω—ã–º–∏ —à–∏–ø–∞–º–∏ –∏ —Ä–µ–º–Ω—è–º–∏ –¥–ª—è –∫—Ä–µ–ø–ª–µ–Ω–∏—è. –û–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ —Ç–∞–∫—Ç–∏–∫—É –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Å–Ω–µ–∂–Ω–æ-–ª–µ–¥–æ–≤–æ–º—É —Å–∫–ª–æ–Ω—É –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–æ–≤.

–° —Ç–µ—Ö –≤—Ä–µ–º—ë–Ω –º–æ–¥–µ–ª–∏ –∑–∞–º–µ—Ç–Ω–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–ª–∏, –Ω–æ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —ç—Ç–æ –∏–∑–¥–µ–ª–∏—è –∏–∑ –º–µ—Ç–∞–ª–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–ø—è—Ç—Å—è –∫ –±–æ—Ç–∏–Ω–∫–∞–º, –≤–≥—Ä—ã–∑–∞—é—Ç—Å—è –≤ –ª—ë–¥ –∏ –¥–µ—Ä–∂–∞—Ç –Ω–∞ —Å–Ω–µ–∂–Ω–æ–º —Ä–µ–ª—å–µ—Ñ–µ'''


# Create Gradio interface
with gr.Blocks(title="RuWordNet Taxonomy Prediction Client", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üîç RuWordNet Taxonomy Prediction Client")
    gr.Markdown("""
    –≠—Ç–æ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ API —Å–µ—Ä–≤–∏—Å—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Å—Ç–∞ –ø–æ–Ω—è—Ç–∏—è –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ RuWordNet.
    
    **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:**
    1. –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: "–†—É—á–Ω–æ–π –≤–≤–æ–¥" –∏–ª–∏ "–î–∞—Ç–∞—Å–µ—Ç"
    2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    3. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã
    4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑
    """)
    
    # Mode selection
    with gr.Tab("üñäÔ∏è –†—É—á–Ω–æ–π –≤–≤–æ–¥"):
        # Text input
        text_input = gr.Textbox(
            label="üìù –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å —Ç–µ–≥–∞–º–∏ <predict_kb>...</predict_kb>",
            lines=10,
            value=example_text
        )
        
        manual_run_btn = gr.Button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ (3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞)", variant="primary", size="lg")
    
    with gr.Tab("üìä –†–µ–∂–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞"):
        with gr.Row():
            dataset_file = gr.File(
                label="üìÅ –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (TSV)",
                file_types=[".tsv"]
            )
            corpus_folder = gr.Textbox(
                label="üìÇ –ü–∞–ø–∫–∞ —Å –∫–æ—Ä–ø—É—Å–æ–º —Ç–µ–∫—Å—Ç–æ–≤",
                value="corpus/private",
                interactive=True
            )
        
        # –ü–∞–ø–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤
        start_nodes_folder = gr.Textbox(
            label="üìÅ –ü–∞–ø–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ (JSON)",
            value="examples",
            interactive=True
        )
        
        start_nodes_info = gr.Textbox(
            label="‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–∞—Ö",
            interactive=False
        )
        
        dataset_info = gr.Textbox(
            label="‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ",
            interactive=False
        )
        
        with gr.Row():
            with gr.Column(scale=3):
                word_dropdown = gr.Dropdown(
                    label="üéØ –ü–æ–∏—Å–∫ –∏ –≤—ã–±–æ—Ä —Å–ª–æ–≤–∞",
                    choices=[],
                    interactive=True,
                    allow_custom_value=True,
                    info="–ù–∞—á–Ω–∏—Ç–µ –ø–µ—á–∞—Ç–∞—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"
                )
            with gr.Column(scale=1):
                word_validation = gr.Textbox(
                    label="‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–≤–∞",
                    interactive=False,
                    lines=2
                )
        
        num_samples = gr.Slider(
            minimum=1,
            maximum=1,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
            value=1,
            step=1,
            label="üî¢ –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è –±–∞—Ç—á-—Ä–µ–∂–∏–º–∞)"
        )
        
        # –¢–µ–∫—Å—Ç –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        word_text_display = gr.Textbox(
            label="üìù –¢–µ–∫—Å—Ç –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞",
            lines=8,
            interactive=False
        )
        
        with gr.Row():
            dataset_run_btn = gr.Button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ", variant="primary")
            batch_run_btn = gr.Button("üîÑ –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞", variant="secondary")
        
        # –ë–∞—Ç—á-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        with gr.Accordion("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏", open=False):
            batch_size = gr.Slider(
                minimum=1,
                maximum=10,
                value=3,
                step=1,
                label="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
            )
        
        batch_results = gr.Textbox(
            label="üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏",
            lines=5,
            interactive=False
        )
    
    # Parameters section (shared)
    with gr.Accordion("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã", open=True):
        with gr.Row():
            # Model parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏**")
                max_iterations = gr.Slider(
                    minimum=5,
                    maximum=100,
                    value=50,
                    step=1,
                    label="–ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π"
                )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.5,
                    step=0.1,
                    label="Temperature"
                )
                top_p = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.95,
                    step=0.05,
                    label="Top P"
                )
            
            # Pipeline parameters
            with gr.Column():
                gr.Markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞**")
                reranking = gr.Checkbox(
                    label="üîÑ –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    value=False
                )
                functions = gr.CheckboxGroup(
                    label="üîß –§—É–Ω–∫—Ü–∏–∏",
                    choices=[
                        ("–ü–æ–ª—É—á–∏—Ç—å –≥–∏–ø–æ–Ω–∏–º—ã", "get_hyponyms"),
                        ("–ü–æ–ª—É—á–∏—Ç—å –≥–∏–ø–µ—Ä–æ–Ω–∏–º—ã", "get_hypernyms")
                    ],
                    value=["get_hyponyms"]
                )
    
    # 3 parallel outputs
    gr.Markdown("### üìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("#### üîµ –ü—Ä–æ—Ü–µ—Å—Å #1")
            process_output_1 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #1",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_1 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #1",
                lines=3,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü¢ –ü—Ä–æ—Ü–µ—Å—Å #2")
            process_output_2 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #2",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_2 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #2",
                lines=3,
                interactive=False
            )
        
        with gr.Column():
            gr.Markdown("#### üü° –ü—Ä–æ—Ü–µ—Å—Å #3")
            process_output_3 = gr.Textbox(
                label="–õ–æ–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ #3",
                lines=15,
                max_lines=20,
                interactive=False
            )
            result_output_3 = gr.Textbox(
                label="–†–µ–∑—É–ª—å—Ç–∞—Ç #3",
                lines=3,
                interactive=False
            )
    
    # Examples
    gr.Markdown("### üìù –ü—Ä–∏–º–µ—Ä—ã")
    gr.Examples(
        examples=[
            [example_text, 50, 0.5, 0.95, False, ["get_hyponyms"]],
            ["–≠—Ç–æ—Ç <predict_kb>–≤–µ–ª–æ—Å–∏–ø–µ–¥</predict_kb> –±—ã–ª –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω –≤ –ì–µ—Ä–º–∞–Ω–∏–∏.", 50, 0.5, 0.95, True, ["get_hyponyms", "get_hypernyms"]],
            ["–ù–æ–≤—ã–π <predict_kb>—Å–º–∞—Ä—Ç—Ñ–æ–Ω</predict_kb> –∏–º–µ–µ—Ç –æ—Ç–ª–∏—á–Ω—É—é –∫–∞–º–µ—Ä—É.", 50, 0.5, 0.95, False, ["get_hyponyms"]],
        ],
        inputs=[text_input, max_iterations, temperature, top_p, reranking, functions],
        label="–ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"
    )
    
    # Info
    gr.Markdown("""
    ---
    ### ‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞:**
    - **–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ** - –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - **–§—É–Ω–∫—Ü–∏–∏** - –≤—ã–±–µ—Ä–∏—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (get_hyponyms, get_hypernyms)
    
    **–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
    - `not_found` - –ø–æ–Ω—è—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
    - `include in {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∏–Ω–æ–Ω–∏–º–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–∏–Ω—Å–µ—Ç–∞
    - `hyponym of {synset_id}` - –ø–æ–Ω—è—Ç–∏–µ —è–≤–ª—è–µ—Ç—Å—è –≥–∏–ø–æ–Ω–∏–º–æ–º (–±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ç–∏–ø–æ–º) —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–Ω—è—Ç–∏—è
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
    - –ó–∞–ø—É—Å—Ç–∏—Ç–µ API: `python api.py`
    """)
    
    # Function to run 3 parallel processes using threads
    def run_parallel_analysis(text, max_iterations, temperature, top_p, reranking, functions):
        """Run 3 parallel analysis processes using threads"""
        
        # Queues to communicate between threads
        queues = [Queue(), Queue(), Queue()]
        
        def run_stream(queue_idx, text, max_iterations, temperature, top_p, reranking, functions):
            """Run streaming in a thread and put results in queue"""
            try:
                for process_log, final_result in process_text_stream(
                    text, max_iterations, temperature, top_p, reranking, functions
                ):
                    queues[queue_idx].put((process_log, final_result))
            except Exception as e:
                queues[queue_idx].put((f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ: {str(e)}", ""))
            finally:
                # Signal completion
                queues[queue_idx].put(None)
        
        # Start 3 threads
        threads = []
        for i in range(3):
            thread = threading.Thread(
                target=run_stream,
                args=(i, text, max_iterations, temperature, top_p, reranking, functions)
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # Track state for each process
        active = [True, True, True]
        results = [("üîµ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #1...", ""), ("üü¢ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #2...", ""), ("üü° –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ #3...", "")]
        
        # Continuously check queues and yield updates
        while any(active):
            updated = False
            
            for i in range(3):
                if active[i]:
                    try:
                        # Try to get item without blocking
                        while not queues[i].empty():
                            item = queues[i].get_nowait()
                            if item is None:
                                active[i] = False
                            else:
                                results[i] = item
                                updated = True
                    except:
                        pass
            
            # Yield current state (even if not updated to keep UI responsive)
            yield (results[0][0], results[0][1],
                   results[1][0], results[1][1],
                   results[2][0], results[2][1])
            
            # Small sleep to prevent busy waiting but keep responsive
            time.sleep(0.05)
        
        # Final yield to ensure all results are shown
        yield (results[0][0], results[0][1],
               results[1][0], results[1][1],
               results[2][0], results[2][1])
    
    # Event handlers
    manual_run_btn.click(
        fn=run_parallel_analysis,
        inputs=[text_input, max_iterations, temperature, top_p, reranking, functions],
        outputs=[process_output_1, result_output_1,
                 process_output_2, result_output_2,
                 process_output_3, result_output_3]
    )
    start_nodes_folder.change(
        fn=get_start_nodes_info,
        inputs=[start_nodes_folder],
        outputs=[start_nodes_info]
    )
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–∞–π–¥–µ—Ä–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
    def safe_get_dataset_info(file):
        if not file:
            return "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω", gr.update(choices=[]), 1
        
        file_path = file.name if hasattr(file, 'name') else str(file)
        return get_dataset_info(file_path)

    dataset_file.change(
        fn=safe_get_dataset_info,
        inputs=[dataset_file],
        outputs=[dataset_info, word_dropdown, num_samples]
    )
    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ª–æ–≤–∞
    word_dropdown.change(
        fn=lambda word, corpus: load_word_text_from_corpus(corpus, word),
        inputs=[word_dropdown, corpus_folder],
        outputs=[word_text_display]
    )
    # –†–µ–∂–∏–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    dataset_run_btn.click(
        fn=process_dataset_item,
        inputs=[dataset_file, corpus_folder, word_dropdown, max_iterations, temperature, top_p, reranking, functions],
        outputs=[process_output_1, result_output_1]
    )
    word_dropdown.change(
        fn=lambda dataset_file, query: search_words_in_dataset(dataset_file.name if dataset_file else "", query) if query else gr.update(),
        inputs=[dataset_file, word_dropdown],
        outputs=[word_dropdown]
    )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    word_dropdown.select(
        fn=lambda dataset_file, word: validate_word_in_dataset(dataset_file.name if dataset_file else "", word),
        inputs=[dataset_file, word_dropdown],
        outputs=[word_validation]
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–Ω–æ–º –≤—ã–±–æ—Ä–µ —Å–ª–æ–≤–∞
    word_dropdown.select(
        fn=lambda word, corpus: load_word_text_from_corpus(corpus, word),
        inputs=[word_dropdown, corpus_folder],
        outputs=[word_text_display]
    )
    
    # –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º max_samples
    batch_run_btn.click(
        fn=process_dataset_batch,
        inputs=[dataset_file, corpus_folder, num_samples, batch_size, max_iterations, 
                temperature, top_p, reranking, functions],
        outputs=[batch_results]
    )
    

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=5003, share=False)