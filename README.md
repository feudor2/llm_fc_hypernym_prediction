# RuWordNet LLM-Enhanced Taxonomy Exploration

Клиент–серверный прототип для автоматического определения положения термина в таксономии RuWordNet на основе контекста.  
Состоит из:

- REST API на FastAPI (`api.py`)
- веб‑клиента на Gradio (`app_gradio.py`)
- вспомогательных утилит (загрузка датасетов, корпуса, стартовых узлов, промптов и т.д.)

## Основные возможности

- Приём текста с разметкой целевого термина через теги `<predict_kb>...</predict_kb>`.
- Интеграция с RuWordNet:
  - поиск гипонимов (`get_hyponyms`)
  - поиск гиперонимов (`get_hypernyms`)
- Управление логикой LLM‑агента:
  - итерационный диалог с LLM и инструментами
  - переранжирование кандидатов с учётом контекста (`Reranker`)
  - интерпретация значения в контексте (`Interpreter`)
- Отслеживание всех выбранных синсетов:
  - сохранение полной истории вызовов функций и финального решения в JSON
- Два режима работы клиента:
  - ручной ввод текста
  - обработка датасета (по словам и батч‑режим)

---

## Архитектура

### API (`api.py`)

Сервис FastAPI предоставляет:

- `POST /predict`  
  Синхронное получение финального результата (без пошагового стриминга).
- `POST /predict/stream`  
  SSE‑стриминг процесса рассуждения и вызовов инструментов.
- `GET /health`  
  Проверка состояния сервиса и загрузки RuWordNet.

Основные компоненты:

- `RuWordNet('wordnets/RuWordNet')` — загрузка таксономии.
- `AsyncOpenAI` — LLM‑клиент (API‑ключ и базовый URL читаются из `.env`).
- `Reranker`, `Interpreter` — вспомогательные промпт‑функции для:
  - получения смысловой интерпретации контекста,
  - переранжирования гипо-/гиперонимов по релевантности.
- `SynsetTracker` — трекинг:
  - целевого слова,
  - всех синсетов, фигурирующих в ходе работы (инструменты + финальный результат),
  - сохранение отчётов в JSON.

Процесс предсказания:

1. Из текста извлекается целевое слово по тегам `<predict_kb>...</predict_kb>`.
2. (Опционально) вычисляется интерпретация значения в контексте (`Interpreter`).
3. (Опционально) добавляется контекст стартового узла (`start_node_id`):
   - имя синсета, его слова,
   - релевантные гипонимы/гиперонимы в виде отформатированного текста.
4. Формируется системный промпт в зависимости от списка доступных функций:
   - только гипонимы,
   - только гиперонимы,
   - оба типа.
5. LLM по итерациям вызывает инструменты `get_hyponyms`/`get_hypernyms`.
6. Инструменты обращаются к RuWordNet, при необходимости запускают переранжирование по целевому слову.
7. Результаты и выбранные синсеты записываются в `SynsetTracker`.
8. После финального ответа:
   - извлекается id целевого синсета (если есть),
   - трекер дополняется финальным решением,
   - при указании `output_file` всё сохраняется на диск.

Формат ответа `/predict`:

```json
{
  "result": "строка с финальным выводом",
  "iterations": 10,
  "full_conversation": [...],
  "tracking_data": {
    "target_word": "...",
    "selected_synsets": [
      {"synset_id": "...", "function": "get_hyponyms", "args": {...}, "timestamp": "..."},
      ...
    ],
    "total_selections": 5,
    "final_result": "...",
    "iterations": 10,
    "timestamp": "..."
  }
}
```

### Клиент (`app_gradio.py`)

Gradio‑интерфейс для взаимодействия с API (по умолчанию `http://localhost:8500`).

#### Ручной режим

- Текстовое поле для ввода контекста с тегами `<predict_kb>...</predict_kb>`.
- Настройки:
  - `max_iterations`
  - `temperature`
  - `top_p`
  - `Переранжирование` (включает/выключает более точный выбор синсетов)
  - `Семантический анализ` (извлечение смысла для контекстуализированного переранжирования)
  - список функций:
    - `get_hyponyms`
    - `get_hypernyms`
  - количество параллельных процессов (1–3)
  - количество стартовых узлов (0–3, при наличии файла стартовых узлов)
- Опциональный путь к файлу отслеживания для ручного анализа.

Кнопка запуска:
- Отправляет до трёх параллельных запросов к `/predict/stream`.
- Каждый поток отображается в своём столбце:
  - лог работы (итерации, мысли LLM, вызовы инструментов, ответы инструментов, итог),
  - итоговый результат.

#### Режим датасета

Поддерживает сценарий массовой оценки качества:

- Загрузка:
  - файла датасета (TSV) со списком целевых слов и путей к текстам корпуса,
  - папки корпуса текстов,
  - файла стартовых узлов (JSON) для слов (например, с базлайна fastText).
- Информация:
  - общее количество слов в датасете,
  - количество доступных стартовых узлов.
- Выбор слова:
  - поиск по словарю (автодополнение),
  - валидация того, что слово есть в датасете,
  - отображение связанного текста (если несколько — переключение стрелками).
- Параметры батч‑обработки:
  - начальный индекс в датасете,
  - количество слов для обработки,
  - размер батча,
  - режим параллелизма:
    - по текстам (1 текст = 1 значение слова),
    - по стартовым узлам (если загружены),
    - стандартный (несколько запусков сэмплирования на один текст).
- Два варианта запуска:
  - обработка выбранного слова (стриминг в три параллельных процесса),
  - асинхронная батч‑обработка подмножества датасета.

Результаты батч‑режима:

- Для каждого задания сохраняется JSON трекинга в `tracking_results/batch_...`.
- Итоговый сводный JSON по словам сохраняется в `test_results/async_batch_...json`.
- В интерфейсе показывается краткая сводка (количество задач, количество слов, путь к файлу).

---

## Установка и запуск

### 1. Клонирование репозитория

```bash
git clone https://github.com//feudor2/llm_fc_hypernym_prediction.git
cd llm_fc_hypernym_prediction
```

### 2. Установка зависимостей

Рекомендуется использовать виртуальное окружение (`venv`, `conda` и т.д.).

```bash
pip install -r requirements.txt
```

### 3. Подготовка данных

- Таксономия RuWordNet должна быть доступна по пути:

  ```text
  wordnets/RuWordNet
  ```

- Датасеты и корпус:
  - TSV‑файл с разметкой датасета (слово → список названий синсетов  →  список ID синсетов  →  список имен файлов из корпуса).
  
    ```text
    datasets/context_analyser_dataset.tsv
    ```
    
  - Папка корпуса текстов, например:

    ```text
    corpus/annotated_texts
    ```

- Файл стартовых узлов (опционально):

  ```text
  examples/fasttext_baseline.json
  ```
  
  Формат — словарь: слово → список ID синсетов

### 4. Настройка окружения

Создайте файл `.env` в корне проекта и укажите:

```env
API_KEY=ваш_openai_api_key
BASE_URL=базовый_URL_совместимого_API
MODEL_NAME=имя_модели
```

### 5. Запуск API

```bash
python -m app.api
```

По умолчанию сервис поднимается на `http://localhost:8500`

Проверка:

```bash
curl http://localhost:8500/health
```

Должен вернуться JSON со статусом `ok` и информацией о загрузке RuWordNet.

### 6. Запуск клиента

В отдельном терминале:

```bash
python -m app.app_gradio
```

По умолчанию Gradio запускается на:

```text
http://127.0.0.1:5003
```

В интерфейсе уже зашит URL API `http://localhost:8500`.

---

## Протокол взаимодействия

### Формат запроса к `/predict` и `/predict/stream`

```json
{
  "text": "Контекст с <predict_kb>целевым словом</predict_kb>...",
  "max_iterations": 50,
  "temperature": 0.5,
  "top_p": 0.95,
  "reranking": true,
  "interpreting": true,
  "functions": ["get_hyponyms", "get_hypernyms"],
  "output_file": "tracking_results/example.json",
  "start_node_id": "12345-N"
}
```

Обязательное условие: в `text` должны быть теги `<predict_kb>...</predict_kb>`, иначе запрос вернётся с ошибкой.

### Стриминговый ответ (`/predict/stream`)

Возвращается в формате `text/event-stream` (SSE). Типы событий:

- `iteration` — начало очередной итерации LLM.
- `thought` — текст ответа модели на данной итерации.
- `tool_call` — информация о вызове инструмента (имя функции, аргументы, синсет/узел).
- `tool_response` — отформатированный ответ инструмента (markdown).
- `tracking_saved` — уведомление о том, что JSON с трекингом сохранён.
- `final` — финальный результат и статистика трекинга.
- `error` — сообщение об ошибке.

Клиент `app_gradio.py`:

- разбирает эти события построчно,
- аккумулирует лог процесса и финальный результат,
- отображает их в текстовых блоках.

---

## Логирование и отладка

- Логи API пишутся в папку `logs/` с ротацией по дате:
  - уровень — `DEBUG` (можно изменить в `logging.basicConfig`).
- В логах подробно фиксируются:
  - исходные сообщения в диалог с моделью,
  - ответы модели,
  - вызовы инструментов и их результаты,
  - сохранение файлов трекинга,
  - ошибки при обращении к RuWordNet или LLM API.

---

## Расширение и модификация

- Добавить новые инструмент‑функции (например, для других типов отношений RuWordNet):
  - реализовать асинхронную функцию по аналогии с `get_hyponyms` / `get_hypernyms`,
  - зарегистрировать её в `available_tools` и в `available_tools_config` (в `utils.tools`),
  - при необходимости дополнить Gradio‑интерфейс чекбоксом.
- Изменить тип используемой LLM:
  - скорректировать переменные окружения в `.env`,
  - при необходимости поправить параметры `max_tokens`, `temperature` и т.п.
- Настроить другие стратегии параллелизма и батч‑обработки в `process_dataset_batch_async`.

---

## Запуск минимального примера (без Gradio)

После запуска API можно обратиться напрямую:

```bash
curl -X POST http://localhost:8500/predict \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Этот <predict_kb>велосипед</predict_kb> был изготовлен в Германии.",
    "max_iterations": 30,
    "temperature": 0.5,
    "top_p": 0.95,
    "reranking": true,
    "interpreting": true,
    "functions": ["get_hyponyms", "get_hypernyms"]
  }'
```

Либо использовать UI Gradio и примеры, уже встроенные в интерфейс.
